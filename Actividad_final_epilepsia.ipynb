{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Actividad_final_epilepsia.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN9vepvsxn7K5xCKfDGObJf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SemilleroNeuroCo/actividad_final_epilepsia/blob/main/Actividad_final_epilepsia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDH_gDDfJpqE"
      },
      "source": [
        "#**Predicción de episodios epilépticos**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz6iq3d-hsRy"
      },
      "source": [
        "http://epileptologie-bonn.de/cms/front_content.php?idcat=193&lang=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE96IGtyZnzx"
      },
      "source": [
        "###Librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFhNlXFDXUVX"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6WthZ3jZuub"
      },
      "source": [
        "###Librerías Modelos ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivaUFJ-2Zs1H"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4XLC6LaNEXm"
      },
      "source": [
        "## Conexión con Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBAzd5_kNC9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3114182-0a9d-43b3-cc64-15f6a4cd7e1e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv8vwovsKJir"
      },
      "source": [
        "##Librería PyEEG\n",
        "\n",
        "Repositorio --> https://github.com/forrestbao/pyeeg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7tfBui-KN9_"
      },
      "source": [
        "# Change directory\n",
        "\n",
        "# ///////////     1) Copie la ruta de la carpeta donde va a trabajar    ////////\n",
        "\n",
        "%cd /content/drive/My Drive/---    \n",
        "# Clone the entire repo.\n",
        "\n",
        "# ///////////     2) Complete copiando la url del repositorio y asignando el nombre a la carpeta clonada    ////////\n",
        "\n",
        "!git clone -l -s https://github.com/---- [Asignar nombre a la carpeta clonada]\n",
        "\n",
        "# ///////////     3) Verificar si la librería importa correctamente     ////////\n",
        "\n",
        "import pyeeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2C5tSIqZ26F"
      },
      "source": [
        "#Importación de los sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duSeUphaZLlg"
      },
      "source": [
        "%cd /content/drive/My Drive/---\n",
        "\n",
        "# Step 1: get a list of all csv files in target directory\n",
        "\n",
        "\n",
        "# //////////   4) Complete las rutas    /////////\n",
        "\n",
        "\n",
        "my_dirA = \"/content/drive/My Drive/------/dataset_epilepsia/Z_A\"    \n",
        "my_dirB = \"/content/drive/My Drive/------/dataset_epilepsia/O_B\"\n",
        "my_dirC = \"/content/drive/My Drive/------/dataset_epilepsia/N_C\"\n",
        "my_dirD = \"/content/drive/My Drive/------/dataset_epilepsia/F_D\"\n",
        "my_dirE = \"/content/drive/My Drive/------/dataset_epilepsia/S_E\"\n",
        "\n",
        "list_files = ['filesListA','filesListB','filesListC','filesListD','filesListE']\n",
        "\n",
        "# //////////    5) Cree una lista con las rutas de las carpetas que contiene los dataset   ////////\n",
        "\n",
        "list_dir = [----,-----,-----,-----,----]\n",
        "\n",
        "sets = ['setA','setB','setC','setD','setE']\n",
        "filelist = []\n",
        "\n",
        "for i in range(#):    #--> Este FOR reorre las carpetas de los dataset\n",
        "    list_files[i] = []\n",
        "    os.chdir( list_dir[i] )\n",
        "  \n",
        "    # Step 2: Build up list of files:\n",
        "    for files in glob.glob(\"*.*\"):\n",
        "        fileName, fileExtension = os.path.splitext(files)\n",
        "        filelist.append(fileName) #filename without extension\n",
        "        list_files[i].append(files) #filename with extension\n",
        "        \n",
        "    sets[i] = (pd.concat([pd.read_csv(item, names=[item[:-4]]) for item in list_files[i]], axis=1))\n",
        "\n",
        "#DataFrame con todos los archivos del dataset concatenados:\n",
        "sets_df = pd.concat(sets, axis=1)   \n",
        "\n",
        "print(sets_df)\n",
        "type(sets_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvv3Un4raC49"
      },
      "source": [
        "#Extracción de características"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0foVUxqhZL5z"
      },
      "source": [
        "# ///////////     6) Copie la ruta de la carpeta creada con el repositorio clonado    ////////\n",
        "\n",
        "%cd /content/drive/My Drive/----\n",
        "\n",
        "df_set = [sets[0], sets[1], sets[2], sets[3], sets[4]]\n",
        "list_et = ['Sano','Sano','Transición','Transición','Ataque']\n",
        "\n",
        "list_etiqueta = []\n",
        "DFAs = []; HFDs = []; SVDes = []; FIs = []; PFDs = []\n",
        "\n",
        "for d in range(5):    \n",
        "    for name_col in filelist[d*100:(d*100+100)]:\n",
        "        data_canal = df_set[d][name_col]\n",
        "        data_canal = data_canal.astype(np.float64)\n",
        "        list_etiqueta.append(list_et[d])\n",
        "\n",
        "# Cálculo de las características con PyEEG:        \n",
        "\n",
        "#        #Higuchi Fractal Dimension\n",
        "        HFD = pyeeg.hfd(data_canal,5); HFDs.append(HFD)\n",
        "#        #Singular Value Decomposition Entropy\n",
        "        SVDe = pyeeg.entropy.svd_entropy(data_canal,1,2); SVDes.append(SVDe)\n",
        "#        #Fisher Information\n",
        "        FI = pyeeg.fisher_info(data_canal,1,2); FIs.append(FI)\n",
        "\n",
        "\n",
        "# ///////////     7) Calcule las últimas dos características    ////////\n",
        "////////// #[nota: el único argumentos de estas dos características es la data ]    ////////\n",
        "\n",
        "        #Detrended Fluctuation Analysis\n",
        "        DFA = ------; DFAs.append(DFA)\n",
        "#        #Petrosian Fractal Dimension\n",
        "        PFD = ------; PFDs.append(PFD)\n",
        "\n",
        "\n",
        "print(DFAs)\n",
        "len(DFAs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZWs_y6faJHB"
      },
      "source": [
        "#Separación por pacientes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDsl3tkqZXBb"
      },
      "source": [
        "# ///////////     8) Grafique las características calculadas y observe la variación de los datos con las etiquetas    ////////\n",
        "\n",
        "Data_total = pd.DataFrame({'DFA':DFAs,'HFD':HFDs,'SVDe':SVDes,'FI':FIs,'PFD':PFDs,'Categorías':list_etiqueta})\n",
        "\n",
        "Data_total.boxplot(by='Categorías',column=['DFA'], grid=True)\n",
        "plt.title('Detrended Fluctuation Analysis')\n",
        "plt.suptitle(' ')\n",
        "\n",
        "Data_total.boxplot(by='Categorías',column=['HFD'], grid=True)\n",
        "plt.title('Higuchi Fractal Dimension')\n",
        "plt.suptitle(' ')\n",
        "\n",
        "Data_total.boxplot(by='Categorías',column=['SVDe'], grid=True)\n",
        "plt.title('Singular Value Decomposition Entropy')\n",
        "plt.suptitle(' ')\n",
        "\n",
        "Data_total.boxplot(by='Categorías',column=['FI'], grid=True)\n",
        "plt.title('Fisher Information')\n",
        "plt.suptitle(' ')\n",
        "\n",
        "Data_total.boxplot(by='Categorías',column=['PFD'], grid=True)\n",
        "plt.title('Petrosian Fractal Dimension')\n",
        "plt.suptitle(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5fOBl36aVY8"
      },
      "source": [
        "#Clasificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh9FpadmZa0c"
      },
      "source": [
        "#División training-test\n",
        "# Split-out validation dataset\n",
        "col = ['DFA','HFD','SVDe','FI','PFD','Categorías']\n",
        "\n",
        "X = Data_total[col[:5]]     # ///////////     9)  Seleccione las columnas de características    ////////\n",
        "y = Data_total[col[#]]      # ///////////    10)  Seleccione la columna categorías    ////////\n",
        "                   \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
        "\n",
        "# ///////////    11)  Complete las líneas de código para los diferentes modelos y analice los accuracy de cada uno   ////////\n",
        "\n",
        "# Models: K-Nearest Neighbour(KNN)\n",
        "model1 = ---()\n",
        "model1.fit(--,--)\n",
        "predictions = model1.predict(--)\n",
        "print(accuracy_score(Y_test,predictions))\n",
        "matriz_KNN = confusion_matrix(Y_test, predictions)\n",
        "print(matriz_KNN)\n",
        "\n",
        "# Models: Support Vector Machine\n",
        "model2 = ---()\n",
        "model2.fit(--,--)\n",
        "predictions = model2.predict(--)\n",
        "print(accuracy_score(Y_test,predictions))\n",
        "matriz_SVC = confusion_matrix(Y_test, predictions)\n",
        "print(matriz_SVC)\n",
        "\n",
        "# Models: Logistic Regression\n",
        "model3 = ---()\n",
        "model3.fit(--,--)\n",
        "predictions = model3.predict(--)\n",
        "print(accuracy_score(Y_test,predictions))\n",
        "\n",
        "# Matriz de confusión\n",
        "matriz_LR = confusion_matrix(Y_test, predictions)\n",
        "print(matriz_LR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3kdHXbs1DVj"
      },
      "source": [
        "# Escalado de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz3QVZzM1A8p"
      },
      "source": [
        "# ///////////    12) Analice el escalado de los datos y compare con los datos originales [sets_df]  ////////\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler_data = scaler.fit_transform(sets_df)\n",
        "print(scaler_data)\n",
        "scaler_data = scaler_data.T\n",
        "print(scaler_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy14wK_3pzAe"
      },
      "source": [
        "# Creación de etiquetas \n",
        "**0** --> Healthy          \n",
        "**1** --> Inter-ictal         \n",
        "**2** --> Ictal             "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8gFOcFHEv0u"
      },
      "source": [
        "# ///////////    13) Ejecute esta celda de código y observe la variable Etiquetas  //////////\n",
        "\n",
        "etiquetas = [0]*200 + [1]*200+[2]*100\n",
        "etiquetas = np.array(etiquetas)\n",
        "print(etiquetas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SBxjRQj7fBa"
      },
      "source": [
        "# Red Neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTnR4uJT7eS6"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ///////////    14) Para las variables X y Y, asigne los valores escalados y las etiquetas, respectivamente.   //////////\n",
        "\n",
        "X = ----\n",
        "y = ----\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1, test_size=0.2)\n",
        "\n",
        "# ///////////    15) Asigne el tamaño de las capas ocultas de la red neuronal con: 2048, 1024 y 512. Además, usar la función de activación ReLU y un máximo de 500 iteraciones  //////////\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(--, --, --), solver='adam', random_state=1)\n",
        "\n",
        "# ///////////    16) Entrene la red con el set de entrenamiento  //////////\n",
        "\n",
        "mlp.fit(--,--)\n",
        "\n",
        "predict_test = mlp.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVuR5y4qookv"
      },
      "source": [
        "# Métricas Scikitlearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwtb6FBw_CcS"
      },
      "source": [
        "# ///////////    17) Analices las siguientes métricas para la red neuronal y compare con los resultados de los modelos de Machine Learnig  //////////"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oozf7T5donAQ"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_WRUXieoLB3"
      },
      "source": [
        "##Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgzpQzCdoP58"
      },
      "source": [
        "print(accuracy_score(y_test,predict_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7SRZbk5oUDP"
      },
      "source": [
        "## Clasification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKlzM7-XodYs"
      },
      "source": [
        "print(classification_report(y_test,predict_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A_aC3YaogtY"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-NrkQr4L1Wy"
      },
      "source": [
        "print(confusion_matrix(y_test,predict_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}